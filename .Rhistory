qplot(seq_along(CompressiveStrength), CompressiveStrength, colour=Age, data = training)
qplot(seq_along(CompressiveStrength), CompressiveStrength, colour=Cement, data = training)
qplot(seq_along(CompressiveStrength), CompressiveStrength, colour=BlastFurnaceSlag, data = training)
qplot(seq_along(CompressiveStrength), CompressiveStrength, colour=Superplasticizer, data = training)
qplot(seq_along(CompressiveStrength), CompressiveStrength, colour=Age, data = training)
library(AppliedPredictiveModeling)
data(concrete)
library(caret)
set.seed(975)
inTrain = createDataPartition(mixtures$CompressiveStrength, p = 3/4)[[1]]
training = mixtures[ inTrain,]
testing = mixtures[-inTrain,]
hist(training$SuperPlasticizer)
summary(training)
hist(training$Superplasticizer)
hist(log(training$Superplasticizer)
)
hist(training$Superplasticizer)
summary(training$Superplasticizer)
summary(log(training$Superplasticizer))
library(caret)
library(AppliedPredictiveModeling)
set.seed(3433)
data(AlzheimerDisease)
adData = data.frame(diagnosis,predictors)
inTrain = createDataPartition(adData$diagnosis, p = 3/4)[[1]]
training = adData[ inTrain,]
testing = adData[-inTrain,]
names(training)
?prcomp
preProc <- preProcess(log10(spam[,-58]+1),method="pca",pcaComp=2)
spamPC <- predict(preProc,log10(spam[,-58]+1))
plot(spamPC[,1],spamPC[,2],col=typeColor)
library(caret); library(kernlab); data(spam)
inTrain <- createDataPartition(y=spam$type,
p=0.75, list=FALSE)
training <- spam[inTrain,]
testing <- spam[-inTrain,]
preProc <- preProcess(log10(spam[,-58]+1),method="pca",pcaComp=2)
spamPC <- predict(preProc,log10(spam[,-58]+1))
plot(spamPC[,1],spamPC[,2],col=typeColor)
typeColor <- ((spam$type=="spam")*1 + 1)
plot(spamPC[,1],spamPC[,2],col=typeColor)
head(typeColor)
tail(typeColor)
class(preProc)
testPC <- predict(preProc,log10(testing[,-58]+1))
class(testPC)
confusionMatrix(testing$type,predict(modelFit,testPC))
preProc <- preProcess(log10(training[,-58]+1),method="pca",pcaComp=2)
trainPC <- predict(preProc,log10(training[,-58]+1))
modelFit <- train(training$type ~ .,method="glm",data=trainPC)
testPC <- predict(preProc,log10(testing[,-58]+1))
confusionMatrix(testing$type,predict(modelFit,testPC))
?preProcess
library(caret)
library(AppliedPredictiveModeling)
set.seed(3433)
data(AlzheimerDisease)
adData = data.frame(diagnosis,predictors)
inTrain = createDataPartition(adData$diagnosis, p = 3/4)[[1]]
training = adData[ inTrain,]
testing = adData[-inTrain,]
pPobject <- preProcess(training,method="pca",thresh=.90)
IL <- training[,c(58,69)]
sapply(IL,class)
IL <- training[,c(58:69)]
sapply(IL,class)
pPobject <- preProcess(IL,method="pca",thresh=.90)
pProject
pProbject
pPobject
library(caret)
library(AppliedPredictiveModeling)
set.seed(3433)
data(AlzheimerDisease)
adData = data.frame(diagnosis,predictors)
inTrain = createDataPartition(adData$diagnosis, p = 3/4)[[1]]
training = adData[ inTrain,]
testing = adData[-inTrain,]
names(training)
head(training$Genotype)
head(training$diagnosis)
str(training$diagnosis)
ILtraining <- training[,c(1,58:69)]
summary(ILtraining)
model1 <- train(diagnosis ~.,data=ILtraining, method="glm")
model1$finalModel
model1
ILtesting <- testing[,c(1,58:69)]
predictions <- predict(model1,newdata=ILtesting)
confusionMatrix(predictions,ILtesting$diagnosis)
head(ILtraining[-1])
pPobject <- preProcess(ILtraining[-1],method="pca",thresh=.80)
trainPC <- predict(pPobject,ILtraining[-1])
model2 <- train(ILtraining$diagnosis ~ .,method="glm",data=trainPC)
testPC <- predict(pPobject,ILtesting[-1])
confusionMatrix(ILtesting$diagnosis,predict(model2,testPC))
data(iris); library(ggplot2)
names(iris)
table(iris$Species)
inTrain <- createDataPartition(y=iris$Species,
p=0.7, list=FALSE)
library(caret)
inTrain <- createDataPartition(y=iris$Species,
p=0.7, list=FALSE)
qplot(Petal.Width,Sepal.Width,colour=Species,data=training)
training <- iris[inTrain,]
testing <- iris[-inTrain,]
qplot(Petal.Width,Sepal.Width,colour=Species,data=training)
modFit <- train(Species ~ .,method="rpart",data=training)
print(modFit$finalModel)
library(rattle)
install.packages("rattle", dependencies=TRUE)
library(rattle)
fancyRpartPlot(modFit$finalModel)
print(modFit$finalModel)
ss <- sample (1:10, replace=T)
ss
library(AppliedPredictiveModeling)
data(segmentationOriginal)
library(caret)
names(segmentationOriginal)
inTrain <- createDataPartition(y=segmentationOriginal$Case,
p=0.7, list=FALSE)
training <- segmentationOriginal[inTrain,]
testing <- segmentationOriginal[-inTrain,]
set.seed(125)
modFit <- train(Case ~ .,method="rpart",data=segmentationOriginal)
library(rattle)
fancyRpartPlot(modFit$finalModel)
modFit <- train(Case ~ .,method="rpart",data=training)
set.seed(125)
modFit <- train(Case ~ .,method="rpart",data=training)
fancyRpartPlot(modFit$finalModel)
head(training$Case)
modFit$finalModel
inTrain <- createDataPartition(y=segmentationOriginal$Case,
, list=FALSE)
training <- segmentationOriginal[inTrain,]
testing <- segmentationOriginal[-inTrain,]
set.seed(125)
modFit <- train(Case ~ .,method="rpart",data=training)
modFit$finalModel
fancyRpartPlot(modFit$finalModel)
set.seed(125)
inTrain <- createDataPartition(y=segmentationOriginal$Case,
p=0.5, list=FALSE)
training <- segmentationOriginal[inTrain,]
testing <- segmentationOriginal[-inTrain,]
modFit <- train(Case ~ .,method="rpart",data=training)
fancyRpartPlot(modFit$finalModel)
modFit <- train(Class ~ .,method="rpart",data=training)
fancyRpartPlot(modFit$finalModel)
args(trainControl)
args(train.default)
?LOOCV
?trainControl
library(pgmm)
data(olive)
olive = olive[,-1]
install.packages("pgmm")
library(pgmm)
data(olive)
olive = olive[,-1]
names(olive)
modFit <- train(Area ~ .,method="rpart",data=olive)
fancyRpartPlot(modFit$finalModel)
newdata = as.data.frame(t(colMeans(olive)))
newdata
head(olive$Area,20)
head(olive$Area,50)
str(olive)
levels(olive$Area)
unique(olive$Area)
predict(modFit,newdata=newdata)
library(ElemStatLearn)
data(SAheart)
set.seed(8484)
train = sample(1:dim(SAheart)[1],size=dim(SAheart)[1]/2,replace=F)
trainSA = SAheart[train,]
testSA = SAheart[-train,]
install.packages("ElemStatLearn")
library(ElemStatLearn)
data(SAheart)
set.seed(8484)
train = sample(1:dim(SAheart)[1],size=dim(SAheart)[1]/2,replace=F)
trainSA = SAheart[train,]
testSA = SAheart[-train,]
set.seed(13234)
names(trainSA)
modelFit <- train(chd ~ age+alcohol+obesity+tobacco+typea+ldl,data=trainSA, method="glm", family="binomial")
library(caret)
modelFit <- train(chd ~ age+alcohol+obesity+tobacco+typea+ldl,data=trainSA, method="glm", family="binomial")
prediction <- predict(modFit, newdata = testSA)
prediction <- predict(modelFit, newdata = testSA)
dim(prediction)
class(prediction)
length(prediction)
missClass = function(testSA$chd,prediction){sum(((prediction > 0.5)*1) != testSA$chd)/length(testSA$chd)}
head(testSA$chd)
head(prediction)
missClass = function(values,prediction){sum(((prediction > 0.5)*1) != values)/length(values)}
missClass(testSA$chd, prediction)
prediction <- predict(modFit, newdata = trainSA)
prediction <- predict(modelFit, newdata = trainSA)
missClass(trainSA$chd, prediction)
library(ElemStatLearn)
data(vowel.train)
data(vowel.test)
names(vowel.train)
class(vowel.train$y)
head(vowel.train$y)
unique(vowel.train$y)
vowel.train$y <- as.factor(vowel.train$y)
unique(vowel.train$y)
vowel.test$y <- as.factor(vowel.test$y)
set.seed(33833)
?varImp
set.seed(33833)
modFit <- train(y ~ .,data=vowel.train,method="rf",prox=TRUE)
varImp(modFit, ...)
varImp(modFit)
?randomForest
modFit <- randomForest(y ~ ., data=vowel.train, importance=TRUE,
proximity=TRUE)
varImp(modFit)
round(importance(modFit), 2)
modFit <- train(y ~ .,data=vowel.train,method="rf",prox=TRUE, importance=TRUE)
varImp(modFit, "type=2")
varImp(modFit, type=2)
modFit$finalModel$importance
modFit <- randomForest(y ~ ., data=vowel.train, importance=TRUE,
proximity=TRUE)
varImp(modFit, type=2)
order(varImp(modFit, type=2))
?order
imp <- varImp(modFit, type=2)
dim(imp)
order(imp,decreasing = T)
library(AppliedPredictiveModeling)
data(segmentationOriginal)
library(caret)
inTrain <- createDataPartition(y=segmentationOriginal$Case,
p=0.7, list=FALSE)
training <- iris[inTrain,]
testing <- iris[-inTrain,]
set.seed(125)
modFit <- train(Class ~ .,method="rpart",data=training)
names(training)
training <- segmentationOriginal[inTrain,]
testing <- segmentationOriginal[-inTrain,]
set.seed(125)
modFit <- train(Class ~ .,method="rpart",data=training)
fancyRpartPlot(modFit$finalModel)
library(rattle)
fancyRpartPlot(modFit$finalModel)
set.seed(125)
inTrain <- createDataPartition(y=segmentationOriginal$Case,
p=0.7, list=FALSE)
training <- segmentationOriginal[inTrain,]
testing <- segmentationOriginal[-inTrain,]
modFit <- train(Class ~ .,method="rpart",data=training)
fancyRpartPlot(modFit$finalModel)
inTrain <- createDataPartition(y=segmentationOriginal$Case,
p=0.5, list=FALSE)
training <- segmentationOriginal[inTrain,]
testing <- segmentationOriginal[-inTrain,]
modFit <- train(Class ~ .,method="rpart",data=training)
fancyRpartPlot(modFit$finalModel)
plot(mofFit$finalModel,compress = TRUE)
plot(modFit$finalModel,compress = TRUE)
text(modFit$finalModel, use.n = TRUE,cex=0.5)
text(modFit$finalModel, use.n = TRUE,cex=1)
plot(modFit$finalModel,compress = TRUE)
text(modFit$finalModel, use.n = TRUE,cex=0.7)
fancyRpartPlot(modFit$finalModel)
inTrain <- createDataPartition(y=segmentationOriginal$Case, list=FALSE)
training <- segmentationOriginal[inTrain,]
head(training$Case)
head(training$Case, 50)
tail(training$Case, 50)
class(segmentationOriginal)
traning <- segmentationOriginal[segmentationOriginal$Case == Train,]
traning <- segmentationOriginal[segmentationOriginal$Case == "Train",]
training <- segmentationOriginal[segmentationOriginal$Case == "Train",]
testing <- segmentationOriginal[segmentationOriginal$Case == "Test",]
1010 + 1009
tail(training$Case, 50)
head(training$Case, 50)
modFit <- train(Class ~ .,method="rpart",data=training)
set.seed(125)
modFit <- train(Class ~ .,method="rpart",data=training)
fancyRpartPlot(modFit$finalModel)
data(iris); library(ggplot2)
names(iris)
table(iris$Species)
?createDataPartition
getwd()
setwd("C:\Users\Ahmad\Dropbox\PD\Practical Machine Learning - Coursera\Project")
setwd("C:\\Users\\Ahmad\\Dropbox\\PD\\Practical Machine Learning - Coursera\\Project")
dir()
?read.csv
mydata <- read.csv(file="pml-training.csv", header=TRUE)
names(mydata)
levels(mydata$classe)
mydatatest <- read.csv(file="pml-testing.csv", header=TRUE)
dim(mydata)
dim(mydatatest)
head(mydatatest$classe)
head(mydata$classe)
names(mydata[-1,])
names(mydata[,-1])
names(mydata[-1])
names(mydata[-160])
names(mydata[-c(1,160)])
names(mydata[-c(1,2,160)])
names(mydata[-c(1,2,160)])
levels(mydata$classe)
inTrain <- createDataPartition(y=mydata$Classe,
p=0.70, list=FALSE)
inTrain <- createDataPartition(y=mydata$Classe, p=0.70, list=FALSE)
names(mydata)
inTrain <- createDataPartition(y=mydata$classe, p=0.70, list=FALSE)
training <- mydata[inTrain,]
testing <- mydata[-inTrain,]
set.seed(33833)
inTrain <- createDataPartition(y=mydata$classe, p=0.70, list=FALSE)
training <- mydata[inTrain,]
testing <- mydata[-inTrain,]
str(mydata[-c(1:2)])
names(mydata[-c(1:2)])
```{r}
modFit <- randomForest(classe ~ ., data=mydata[-c(1:2)], importance=TRUE, proximity=TRUE)
?randomForest
```{r}
modFit <- randomForest(classe ~ ., data=mydata[-c(1:2)], importance=TRUE, proximity=TRUE, na.action=na.omit)
str(mydata[-c(1:2)])
names(mydata[-160])
names(mydata[-c(1:2,160)])
mydata[-c(1:2,160)] <- as.numeric(mydata[-c(1:2,160)])
mydata[-c(1:2,160)] <- as.numeric(as.character(mydata[-c(1:2,160)]))
str(mydata[-c(1:2)])
mydata <- read.csv(file="pml-training.csv", header=TRUE)
set.seed(33833)
inTrain <- createDataPartition(y=mydata$classe, p=0.70, list=FALSE)
training <- mydata[inTrain,]
testing <- mydata[-inTrain,]
mydata <- mydata[!complete.cases(mydata),]
mydata[-c(1:2,160)] <- as.numeric(mydata[-c(1:2,160)])
sapply(mydata,class)
sapply(mydata[-c(1:2,160)],class)
sapply(mydata[-c(1:2,160)],as.numeric)
str(mydata[-c(1:2)])
mydata[-c(1:2,160)] <- as.numeric(mydata[-c(1:2,160)])
mydata[-c(1:2,160)] <- mydata[as.numeric(-c(1:2,160))]
str(mydata[-c(1:2)])
mydata[ , c(1,2,160)<- sapply(mydata[ , -c(1,2,160)], as.numeric)
mydata[ , -c(1,2,160)]<- sapply(mydata[, -c(1,2,160)], as.numeric)
str(mydata[-c(1:2,160)])
str(mydata[c(1:2,160)])
modFit <- randomForest(classe ~ ., data=mydata[-c(1:2)], importance=TRUE, proximity=TRUE, na.action=na.omit)
str(mydata$classe)
mydata <- read.csv(file="pml-training.csv", header=TRUE)
set.seed(33833)
inTrain <- createDataPartition(y=mydata$classe, p=0.70, list=FALSE)
training <- mydata[inTrain,]
testing <- mydata[-inTrain,]
mydata[ , -c(1,2,160)]<- sapply(mydata[, -c(1,2,160)], as.numeric)
str(mydata$classe)
mydata <- read.csv(file="pml-training.csv", header=TRUE)
set.seed(33833)
inTrain <- createDataPartition(y=mydata$classe, p=0.70, list=FALSE)
training <- mydata[inTrain,]
testing <- mydata[-inTrain,]
mydata[,c(3:159)] <- sapply(mydata[,c(3:159)],as.numeric)
str(mydata$classe)
mydata <- read.csv(file="pml-training.csv", header=TRUE)
str(mydata$classe)
head(mydata$classe)
tail(mydata$classe)
head(mydata[,160])
mydata[,c(3:159)] <- sapply(mydata[,c(3:159)],as.numeric)
head(mydata[,160])
tail(mydata$classe)
set.seed(33833)
inTrain <- createDataPartition(y=mydata$classe, p=0.70, list=FALSE)
training <- mydata[inTrain,]
testing <- mydata[-inTrain,]
modFit <- randomForest(classe ~ ., data=training[c(3:160)], importance=TRUE, proximity=TRUE, na.action=na.omit)
varImp(modFit, type=2)
varImp(modFit, type=2)[1]
varImp(modFit, type=2)[2]
order(varImp(modFit, type=2),decreasing=T)
library(randomForest)
imp <- varImp(modFit, type=2)
imp$max <- apply(imp, 1, max)
imp[order(-vi$max),]
imp <- varImp(modFit, type=2)$importance
imp$max <- apply(imp, 1, max)
imp <- varImp(modFit, type=2)$importance
imp
imp <- varImp(modFit)$importance
imp
imp <- varImp(modFit)
imp
modFit <- train(classe ~ .,data=training[c(3:160)],method="rf",prox=TRUE)
varImp(modFit)
?varImp
varImp(modFit, drop=F)
varImp(modFit, type=2)
imp <- varImp(modFit)
rownames(imp$importance)
str(imp)
rownames(imp[ order(-imp[,1]),]$importance)
dim(imp)
rownames(imp$importance[order(-imp$importance[,1]),]$importance)
imp$importance[order(imp$importance[,1]), ]
imp$importance[order(-imp$importance[,1]), ]
dim(imp$importance)
rownames(imp$importance[order(-imp$importance[,1]), ])
imp$importance[order(-imp$importance[,1]), ]
rownames(imp$importance[order(-imp$importance[,1]), ]$importance)
rownames(imp$importance[order(-imp$importance[,1]), ])
class(imp$importance[order(-imp$importance[,1]), ])
class(imp$importance)
str(imp)
newimp <- imp$importance[order(Overall),]
newimp <- imp$importance[order(Overall),]
newimp <- imp$importance[order(imp$importance$Overall),]
newimp
newimp <- imp$importance[order(-imp$importance$Overall),]
newimp
rownames(newimp)
rownames(newimp$importance)
class(newimp$importance)
class(imp$importance)
newimp <- data.frame(imp$importance[order(-imp$importance$Overall),])
class(newimp$importance)
library(caret)
library(randomForest)
training[,c("min_roll_forearm","var_accel_dumbbell","var_roll_belt","stddev_roll_belt","raw_timestamp_part_1","var_total_accel_belt","avg_roll_dumbbell","magnet_dumbbell_z","magnet_dumbbell_y","avg_roll_belt","num_window","avg_roll_forearm","accel_forearm_z","avg_pitch_forearm","roll_dumbbell","var_accel_arm","cvtd_timestamp","amplitude_yaw_arm","accel_dumbbell_y", "amplitude_pitch_arm")]
dim(training[,c("min_roll_forearm","var_accel_dumbbell","var_roll_belt","stddev_roll_belt","raw_timestamp_part_1","var_total_accel_belt","avg_roll_dumbbell","magnet_dumbbell_z","magnet_dumbbell_y","avg_roll_belt","num_window","avg_roll_forearm","accel_forearm_z","avg_pitch_forearm","roll_dumbbell","var_accel_arm","cvtd_timestamp","amplitude_yaw_arm","accel_dumbbell_y", "amplitude_pitch_arm", "classe")])
trainingsubset <- training[,c("min_roll_forearm","var_accel_dumbbell","var_roll_belt","stddev_roll_belt","raw_timestamp_part_1","var_total_accel_belt","avg_roll_dumbbell","magnet_dumbbell_z","magnet_dumbbell_y","avg_roll_belt","num_window","avg_roll_forearm","accel_forearm_z","avg_pitch_forearm","roll_dumbbell","var_accel_arm","cvtd_timestamp","amplitude_yaw_arm","accel_dumbbell_y", "amplitude_pitch_arm", "classe")]
modFit2 <- train(classe ~ .,data=trainingsubset,method="rf",prox=TRUE)
testingsubset <- testing[,c("min_roll_forearm","var_accel_dumbbell","var_roll_belt","stddev_roll_belt","raw_timestamp_part_1","var_total_accel_belt","avg_roll_dumbbell","magnet_dumbbell_z","magnet_dumbbell_y","avg_roll_belt","num_window","avg_roll_forearm","accel_forearm_z","avg_pitch_forearm","roll_dumbbell","var_accel_arm","cvtd_timestamp","amplitude_yaw_arm","accel_dumbbell_y", "amplitude_pitch_arm", "classe")]
pred <- predict(modFit2,testingsubset); testingsubset$predRight <- pred==testing$classe
sum(testingsubset$predRight)
table(pred,testingsubset$classe)
head(pred)
head(testingsubset$classe)
length(pred)
length(testingsubset$classe)
pred <- predict(modFit2,testingsubset);
length(pred)
modFit2 <- train(classe ~ .,data=trainingsubset,method="rf",prox=TRUE)
pred <- predict(modFit2,testingsubset);
length(pred)
testingsubset$predRight <- pred==testingsubset$classe
sum(testingsubset$predRight)
table(pred,testingsubset$classe)
varImp(modFit)
predict(modelFit2, newdata = testingsubset, type = "prob")
predict(modFit2, newdata = testingsubset, type = "prob")
pred <- predict(modFit2, newdata = testingsubset, type = "prob")
summary(pred)
pred <- predict(modFit2, newdata = testingsubset)
summary(pred)
modFit <- randomForest(y ~ ., data=trainingsubset, importance=TRUE, proximity=TRUE)
modFit <- randomForest(classe ~ ., data=trainingsubset, importance=TRUE, proximity=TRUE)
?randomForest
modFit <- randomForest(classe ~ ., data=trainingsubset, importance=TRUE, proximity=TRUE, na.action=na.fail)
modFit <- randomForest(classe ~ ., data=trainingsubset, importance=TRUE, proximity=TRUE, na.action=na.omit)
pred <- predict(modFit, newdata = testingsubset)
summary(pred)
modFit <- randomForest(classe ~ ., data=training, importance=TRUE, proximity=TRUE, na.action=na.omit)
pred <- predict(modFit, newdata = testing)
summary(pred)
testing$predRight <- pred==testing$classe
table(pred,testing$classe)
mydata <- read.csv(file="pml-training.csv", header=TRUE)
set.seed(33833)
inTrain <- createDataPartition(y=mydata$classe, p=0.70, list=FALSE)
training <- mydata[inTrain,]
testing <- mydata[-inTrain,]
modFit <- train(classe ~ .,data=training[c(3:160)],method="rf",prox=TRUE)
varImp(modFit, type=2)
mydata <- read.csv(file="pml-training.csv", header=TRUE)
mydata[,c(3:159)] <- sapply(mydata[,c(3:159)],as.numeric)
set.seed(33833)
inTrain <- createDataPartition(y=mydata$classe, p=0.70, list=FALSE)
training <- mydata[inTrain,]
testing <- mydata[-inTrain,]
modFit <- train(classe ~ .,data=training[c(3:160)],method="rf",prox=TRUE)
varImp(modFit, type=2)
varImpPlot(modFit)
mydatatest <- read.csv(file="pml-testing.csv", header=TRUE)
names(mydatatest)
mydatatestnomissing[,colSums(is.na(mydatatest)) != nrow(mydatatest)]
mydatatestnomissing <- mydatatest[,colSums(is.na(mydatatest)) != nrow(mydatatest)]
names(mydatatestnomissing)
class(names(mydatatestnomissing))
cols <- names(mydatatestnomissing)
cols <- names(mydatatestnomissing[-60])
cols += "classe"
cols[length(cols)+1] <- "classe"
names(cols)
cols
modFit2 <- train(classe ~ .,data=training[cols],method="rf",prox=TRUE)
trainingsubset <- training[,cols]
modFit2 <- train(classe ~ .,data=trainingsubset,method="rf",prox=TRUE)
modFit2 <- randomForest(classe ~ ., data=trainingsubset, importance=TRUE, proximity=TRUE)
